#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†å„ä¸ªå‘è‚²é˜¶æ®µçš„æ¥è§¦è¡°å‡æ›²çº¿åˆ†æè„šæœ¬ - ä¼˜åŒ–ç‰ˆ

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨å‘ç°å„ä¸ªstageå’Œresolutionç›®å½•ä¸‹çš„.coolæ–‡ä»¶
2. æ‰¹é‡è¿è¡Œæ¥è§¦è¡°å‡æ›²çº¿åˆ†æ
3. ç”Ÿæˆå¯¹æ¯”åˆ†æç»“æœ
4. å¹¶è¡Œå¤„ç†ä»¥æé«˜æ•ˆç‡

ä½œè€…ï¼šClaude Code Assistant
æ—¥æœŸï¼š2025-08-29
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
import pandas as pd
import numpy as np # å¼•å…¥numpyç”¨äºç±»å‹æ£€æŸ¥
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

try:
    from contact_decay_analyzer import ContactDecayAnalyzer
    print("æ­£ç¡®å¼•å…¥ContactDecayAnalyzeræ¨¡å—")
except ImportError:
    print("é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ContactDecayAnalyzerï¼Œè¯·æ£€æŸ¥srcç›®å½•æˆ–ç¡®ä¿å…¶åœ¨PYTHONPATHä¸­")
    sys.exit(1)

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# --- æ–°å¢ï¼šJSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•° ---
def convert_numpy_types(obj):
    """
    é€’å½’åœ°å°†å­—å…¸æˆ–åˆ—è¡¨ä¸­çš„NumPyæ•°å€¼ç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ã€‚
    è¿™æ˜¯è§£å†³ 'Object of type int64 is not JSON serializable' é”™è¯¯çš„å…³é”®ã€‚
    """
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(element) for element in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    return obj

class StageWiseDecayProcessor:
    """å‘è‚²é˜¶æ®µæ¥è§¦è¡°å‡æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, data_root_dir: str, output_dir: str, 
                 resolution: str, stages: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            data_root_dir: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            resolution: è¦å¤„ç†çš„åˆ†è¾¨ç‡ç›®å½•å, ä¾‹å¦‚ '100K'
            stages: è¦å¤„ç†çš„å‘è‚²é˜¶æ®µåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å‘ç°çš„é˜¶æ®µ
        """
        self.data_root_dir = Path(data_root_dir)
        self.output_dir = Path(output_dir)
        self.resolution = resolution
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # å‘è‚²é˜¶æ®µ
        self.stages = stages if stages else self._discover_stages()
        self.logger.info(f"å°†è¦å¤„ç†çš„å‘è‚²é˜¶æ®µ: {self.stages}")
        
        # ç»“æœå­˜å‚¨
        self.stage_results = {}
        self.all_decay_profiles = {}
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.output_dir / f"batch_processing_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def _discover_stages(self) -> List[str]:
        """è‡ªåŠ¨å‘ç°æ•°æ®ç›®å½•ä¸­çš„å‘è‚²é˜¶æ®µ"""
        stages = []
        
        try:
            # æŸ¥æ‰¾ä»¥Eæˆ–EXå¼€å¤´çš„ç›®å½•
            for item in self.data_root_dir.iterdir():
                if item.is_dir() and (item.name.startswith('E') or item.name.startswith('EX')):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æŒ‡å®šåˆ†è¾¨ç‡çš„æ’è¡¥æ•°æ®
                    impute_dir = item / 'impute' / self.resolution / 'chunk0'
                    if impute_dir.exists() and any(impute_dir.glob('*.cool')):
                        stages.append(item.name)
                        
            # æŒ‰åç§°æ’åº
            stages.sort(key=lambda x: (len(x), x))
            return stages
            
        except Exception as e:
            self.logger.error(f"å‘ç°å‘è‚²é˜¶æ®µå¤±è´¥: {e}")
            return []
    
    def _get_stage_cool_files(self, stage: str, max_files: Optional[int] = None) -> List[Path]:
        """
        è·å–æŒ‡å®šé˜¶æ®µçš„.coolæ–‡ä»¶åˆ—è¡¨
        
        Args:
            stage: å‘è‚²é˜¶æ®µåç§°
            max_files: æœ€å¤§æ–‡ä»¶æ•°é‡é™åˆ¶
            
        Returns:
            List[Path]: .coolæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        cool_files = []
        
        try:
            # ä½¿ç”¨ self.resolution æ¥æ„å»ºè·¯å¾„
            stage_dir = self.data_root_dir / stage / 'impute' / self.resolution
            # æœç´¢æ‰€æœ‰ chunk* ç›®å½•ä¸‹çš„ .cool æ–‡ä»¶
            cool_files = sorted(stage_dir.glob('chunk*/*.cool'))
            
            if not cool_files:
                self.logger.warning(f"é˜¶æ®µ {stage} åœ¨ {self.resolution} åˆ†è¾¨ç‡ä¸‹æœªæ‰¾åˆ°.coolæ–‡ä»¶: {stage_dir}")
                return []
            
            # é™åˆ¶æ–‡ä»¶æ•°é‡
            if max_files and len(cool_files) > max_files:
                cool_files = cool_files[:max_files]
                self.logger.info(f"é˜¶æ®µ {stage}: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ä¸º {max_files}")
            
            self.logger.info(f"é˜¶æ®µ {stage}: å‘ç° {len(cool_files)} ä¸ª.coolæ–‡ä»¶")
            return cool_files
            
        except Exception as e:
            self.logger.error(f"è·å–é˜¶æ®µ {stage} çš„æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def process_single_file(self, cool_file: Path, stage: str, 
                           output_subdir: str, **kwargs) -> Dict:
        """
        å¤„ç†å•ä¸ª.coolæ–‡ä»¶
        """
        try:
            file_output_dir = self.output_dir / output_subdir / cool_file.stem
            analyzer = ContactDecayAnalyzer(
                cool_path=str(cool_file),
                output_dir=str(file_output_dir),
                log_level='WARNING'
            )
            results = analyzer.run_complete_analysis(
                chrom=kwargs.get('chrom'),
                max_distance=kwargs.get('max_distance'),
                save_plots=kwargs.get('save_plots', True),
                show_plots=False
            )
            results['stage'] = stage
            results['cool_file'] = str(cool_file)
            return results
        except Exception as e:
            return {'success': False, 'error': str(e), 'stage': stage, 'cool_file': str(cool_file)}
    
    def process_stage(self, stage: str, max_files: Optional[int] = None, 
                     parallel: bool = True, max_workers: int = 4, **kwargs) -> Dict:
        """
        å¤„ç†å•ä¸ªå‘è‚²é˜¶æ®µçš„æ‰€æœ‰.coolæ–‡ä»¶
        """
        self.logger.info(f"å¼€å§‹å¤„ç†å‘è‚²é˜¶æ®µ: {stage} @ {self.resolution}")
        cool_files = self._get_stage_cool_files(stage, max_files)
        if not cool_files:
            return {'stage': stage, 'success': False, 'error': 'æœªæ‰¾åˆ°.coolæ–‡ä»¶'}
        
        stage_output_dir = f"stage_{stage}_{self.resolution}"
        results, successful_count, failed_count = [], 0, 0
        
        if parallel and len(cool_files) > 1:
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                future_to_file = {executor.submit(self.process_single_file, cool_file, stage, stage_output_dir, **kwargs): cool_file for cool_file in cool_files}
                for future in as_completed(future_to_file):
                    cool_file = future_to_file[future]
                    try:
                        result = future.result()
                        results.append(result)
                        if result.get('success', False): successful_count += 1
                        else: failed_count += 1; self.logger.error(f"âŒ {cool_file.stem}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    except Exception as e:
                        failed_count += 1; self.logger.error(f"âŒ {cool_file.stem}: æ‰§è¡Œå¼‚å¸¸ - {e}")
                        results.append({'success': False, 'error': str(e), 'stage': stage, 'cool_file': str(cool_file)})
        else:
            for cool_file in cool_files:
                result = self.process_single_file(cool_file, stage, stage_output_dir, **kwargs)
                results.append(result)
                if result.get('success', False): successful_count += 1
                else: failed_count += 1; self.logger.error(f"âŒ {cool_file.stem}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        stage_summary = {
            'stage': stage, 'resolution': self.resolution, 'total_files': len(cool_files),
            'successful': successful_count, 'failed': failed_count,
            'success_rate': successful_count / len(cool_files) if cool_files else 0,
            'results': results, 'output_dir': str(self.output_dir / stage_output_dir)
        }
        
        # *** ä¿®æ­£ç‚¹ï¼šåœ¨ä¿å­˜JSONå‰è¿›è¡Œç±»å‹è½¬æ¢ ***
        stage_summary = convert_numpy_types(stage_summary)
        
        summary_file = self.output_dir / f"stage_{stage}_{self.resolution}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f: json.dump(stage_summary, f, indent=2, ensure_ascii=False, default=str)
        self.logger.info(f"é˜¶æ®µ {stage} å¤„ç†å®Œæˆ: {successful_count}/{len(cool_files)} æˆåŠŸ")
        return stage_summary
    
    # ... (å…¶ä»–å‡½æ•° collect_decay_profiles, create_comparative_plots, run_batch_processing ä¿æŒä¸å˜) ...
    def collect_decay_profiles(self) -> pd.DataFrame:
        """æ”¶é›†æ‰€æœ‰æˆåŠŸåˆ†æçš„è¡°å‡æ›²çº¿æ•°æ®"""
        all_profiles = []
        for stage in self.stages:
            stage_dir = self.output_dir / f"stage_{stage}_{self.resolution}"
            if not stage_dir.exists(): continue
            for profile_file in stage_dir.glob('*/*/decay_profile.csv'):
                try:
                    df = pd.read_csv(profile_file)
                    df['stage'] = stage
                    df['resolution'] = self.resolution
                    df['sample'] = profile_file.parent.name
                    all_profiles.append(df)
                except Exception as e: self.logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {profile_file}: {e}")
        if all_profiles:
            combined_df = pd.concat(all_profiles, ignore_index=True)
            output_file = self.output_dir / f'all_stages_{self.resolution}_decay_profiles.csv'
            combined_df.to_csv(output_file, index=False)
            self.logger.info(f"åˆå¹¶çš„è¡°å‡æ›²çº¿æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            return combined_df
        else:
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•è¡°å‡æ›²çº¿æ•°æ®")
            return pd.DataFrame()

    def create_comparative_plots(self, combined_df: pd.DataFrame) -> List[str]:
        """åˆ›å»ºå„é˜¶æ®µé—´çš„å¯¹æ¯”å›¾è¡¨"""
        # (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œå› ä¸ºå®ƒä¾èµ–äºDataFrameçš„å†…å®¹)
        if combined_df.empty: return []
        plot_files = []
        try:
            stage_colors = plt.cm.Set3(np.linspace(0, 1, len(self.stages)))
            color_map = dict(zip(self.stages, stage_colors))
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            for stage in self.stages:
                stage_data = combined_df[combined_df['stage'] == stage]
                if stage_data.empty: continue
                avg_decay = stage_data.groupby('distance_kb')['contact_frequency'].mean()
                if not avg_decay.empty:
                    ax1.plot(avg_decay.index, avg_decay.values, 'o-', markersize=3, linewidth=2, label=f'é˜¶æ®µ {stage}', color=color_map[stage], alpha=0.8)
                    ax2.loglog(avg_decay.index, avg_decay.values, 'o-', markersize=3, linewidth=2, label=f'é˜¶æ®µ {stage}', color=color_map[stage], alpha=0.8)
            ax1.set_xlabel('åŸºå› ç»„è·ç¦» (kb)'); ax1.set_ylabel('å¹³å‡æ¥è§¦é¢‘ç‡'); ax1.set_title('å„å‘è‚²é˜¶æ®µæ¥è§¦è¡°å‡å¯¹æ¯” (çº¿æ€§åæ ‡)'); ax1.legend(); ax1.grid(True, alpha=0.3)
            ax2.set_xlabel('åŸºå› ç»„è·ç¦» (kb)'); ax2.set_ylabel('å¹³å‡æ¥è§¦é¢‘ç‡'); ax2.set_title('å„å‘è‚²é˜¶æ®µæ¥è§¦è¡°å‡å¯¹æ¯” (åŒå¯¹æ•°åæ ‡)'); ax2.legend(); ax2.grid(True, alpha=0.3)
            plt.tight_layout()
            plot_file = self.output_dir / f'stage_comparison_decay_curves_{self.resolution}.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight'); plot_files.append(str(plot_file)); plt.close()
            self.logger.info(f"ç”Ÿæˆå¯¹æ¯”å›¾è¡¨: {len(plot_files)} ä¸ª")
            return plot_files
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¯¹æ¯”å›¾è¡¨å¤±è´¥: {e}"); return []

    def run_batch_processing(self, max_files_per_stage: Optional[int] = None, parallel: bool = True, max_workers: int = 4, **analysis_kwargs) -> Dict:
        """è¿è¡Œæ‰¹é‡å¤„ç†æµç¨‹"""
        # (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)
        start_time = datetime.now()
        self.logger.info("å¼€å§‹æ‰¹é‡å¤„ç†æ‰€æœ‰å‘è‚²é˜¶æ®µ")
        all_results, total_successful, total_files = {}, 0, 0
        for stage in self.stages:
            stage_result = self.process_stage(stage=stage, max_files=max_files_per_stage, parallel=parallel, max_workers=max_workers, **analysis_kwargs)
            all_results[stage] = stage_result
            total_successful += stage_result.get('successful', 0); total_files += stage_result.get('total_files', 0)
        combined_df = self.collect_decay_profiles()
        comparison_plots = []
        if not combined_df.empty: comparison_plots = self.create_comparative_plots(combined_df)
        end_time = datetime.now(); processing_time = (end_time - start_time).total_seconds()
        final_summary = {
            'start_time': start_time.isoformat(), 'end_time': end_time.isoformat(), 'processing_time_seconds': processing_time,
            'stages_processed': len(self.stages), 'total_files': total_files, 'total_successful': total_successful,
            'overall_success_rate': total_successful / total_files if total_files > 0 else 0, 'stage_results': all_results,
            'output_directory': str(self.output_dir), 'comparison_plots': comparison_plots,
            'combined_data_points': len(combined_df) if not combined_df.empty else 0
        }
        
        # *** ä¿®æ­£ç‚¹ï¼šåœ¨ä¿å­˜JSONå‰è¿›è¡Œç±»å‹è½¬æ¢ ***
        final_summary = convert_numpy_types(final_summary)

        report_file = self.output_dir / f'batch_processing_report_{self.resolution}.json'
        with open(report_file, 'w', encoding='utf-8') as f: json.dump(final_summary, f, indent=2, ensure_ascii=False, default=str)
        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {processing_time:.1f} ç§’"); self.logger.info(f"æœ€ç»ˆæŠ¥å‘Š: {report_file}")
        return final_summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤„ç†å‘è‚²é˜¶æ®µæ¥è§¦è¡°å‡æ›²çº¿åˆ†æ')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('-i', '--input', required=True, help='è¾“å…¥æ•°æ®æ ¹ç›®å½• (åŒ…å«å„ä¸ªstageå­ç›®å½•)')
    parser.add_argument('-o', '--output', required=True, help='è¾“å‡ºç›®å½•')
    # *** æ–°å¢çš„å¿…éœ€å‚æ•° ***
    parser.add_argument('-r', '--resolution', required=True, help='æŒ‡å®šè¦å¤„ç†çš„åˆ†è¾¨ç‡ç›®å½•ï¼Œä¾‹å¦‚ "100K"')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('-s', '--stages', nargs='+', help='æŒ‡å®šè¦å¤„ç†çš„å‘è‚²é˜¶æ®µ (é»˜è®¤å¤„ç†æ‰€æœ‰å‘ç°çš„é˜¶æ®µ)')
    parser.add_argument('-n', '--max-files', type=int, help='æ¯ä¸ªé˜¶æ®µæœ€å¤§å¤„ç†æ–‡ä»¶æ•°é‡')
    parser.add_argument('-w', '--max-workers', type=int, default=4, help='å¹¶è¡Œå¤„ç†çš„æœ€å¤§workeræ•°é‡')
    parser.add_argument('--no-parallel', action='store_true', help='ç¦ç”¨å¹¶è¡Œå¤„ç†')
    
    # åˆ†æå‚æ•°
    parser.add_argument('-c', '--chrom', help='åˆ†ææŒ‡å®šæŸ“è‰²ä½“')
    parser.add_argument('-d', '--max-distance', type=int, help='æœ€å¤§åˆ†æè·ç¦»')
    parser.add_argument('--no-plots', action='store_true', help='ä¸ç”Ÿæˆå›¾è¡¨')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨ï¼Œå¹¶ä¼ å…¥ resolution å‚æ•°
    processor = StageWiseDecayProcessor(
        data_root_dir=args.input,
        output_dir=args.output,
        resolution=args.resolution, # <--- å°† resolution ä¼ é€’ç»™å¤„ç†å™¨
        stages=args.stages
    )
    
    # è¿è¡Œæ‰¹é‡å¤„ç†
    results = processor.run_batch_processing(
        max_files_per_stage=args.max_files,
        parallel=not args.no_parallel,
        max_workers=args.max_workers,
        chrom=args.chrom,
        max_distance=args.max_distance,
        save_plots=not args.no_plots
    )
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†äº† {results['stages_processed']} ä¸ªå‘è‚²é˜¶æ®µ @ {args.resolution} åˆ†è¾¨ç‡")
    print(f"ğŸ“ æˆåŠŸåˆ†æ {results['total_successful']}/{results['total_files']} ä¸ªæ–‡ä»¶")
    print(f"â±ï¸  æ€»è€—æ—¶: {results['processing_time_seconds']:.1f} ç§’")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {results['overall_success_rate']*100:.1f}%")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {results['output_directory']}")

if __name__ == "__main__":
    main()